{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from telethon.sync import TelegramClient\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "import asyncio\n",
    "import json\n",
    "import datetime\n",
    "\n",
    "\n",
    "\n",
    "# Telegram API credentials (replace with your own)\n",
    "api_id = '20070625'\n",
    "api_hash = 'e575003ddfb16cf5a391ed2322e37f6b'\n",
    "\n",
    "# Delete the existing session file if it exists\n",
    "session_file = 'telegram_scraper.session'\n",
    "if os.path.exists(session_file):\n",
    "    os.remove(session_file)\n",
    "    print(f\"Deleted existing session file: {session_file}\")\n",
    "\n",
    "# Initialize the Telegram client\n",
    "client = TelegramClient('telegram_scraper', api_id, api_hash)\n",
    "\n",
    "# List of channels to scrape\n",
    "channels = ['@ZemenExpress', '@Shewabrand', '@gebeyaadama', '@marakibrand', '@AwasMart']\n",
    "\n",
    "# Function to fetch messages from channels\n",
    "async def fetch_messages():\n",
    "    for channel in channels:\n",
    "        print(f\"Fetching messages from: {channel}\")\n",
    "        async for message in client.iter_messages(channel, limit=700):  # Adjust limit as needed\n",
    "            if message.text:  # If the message contains text\n",
    "                yield {\n",
    "                    'channel': channel,\n",
    "                    'sender': message.sender_id,\n",
    "                    # Convert datetime to ISO format string for JSON serialization\n",
    "                    'timestamp': message.date.isoformat(),\n",
    "                    'text': message.text,\n",
    "                }\n",
    "\n",
    "# Main script\n",
    "async def main():\n",
    "    messages = []\n",
    "    async for msg in fetch_messages():\n",
    "        messages.append(msg)\n",
    "    return messages\n",
    "\n",
    "# Connect to Telegram and fetch data\n",
    "async def connect_and_fetch():\n",
    "    # Connect to Telegram\n",
    "    await client.connect()\n",
    "\n",
    "    # Check if the connection is successful\n",
    "    if not await client.is_user_authorized():\n",
    "        await client.send_code_request(+251986284241)  # Replace 'phone' with your phone number\n",
    "        await client.sign_in(+251986284241, input('Enter the code: 55289'))\n",
    "\n",
    "    try:\n",
    "        data = await main()  # Await main() directly\n",
    "    finally:\n",
    "        # Ensure the client is disconnected in the finally block\n",
    "        await client.disconnect()\n",
    "    return data\n",
    "\n",
    "# Run the asynchronous function using asyncio.run\n",
    "data = asyncio.run(connect_and_fetch())\n",
    "\n",
    "# Save data to a file\n",
    "with open('telegram_data.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(data, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Load raw data\n",
    "with open('telegram_data.json', 'r', encoding='utf-8') as f:\n",
    "    raw_data = pd.read_json(f)\n",
    "\n",
    "# Function to clean and preprocess text\n",
    "def preprocess_text(text):\n",
    "    text = re.sub(r'[^\\w\\s።፣፤፥፦፧]', '', text)  # Remove special characters\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Remove extra whitespace\n",
    "    return text.strip()\n",
    "\n",
    "# Apply preprocessing\n",
    "raw_data['cleaned_text'] = raw_data['text'].apply(preprocess_text)\n",
    "\n",
    "# Save preprocessed data\n",
    "raw_data.to_csv('preprocessed_data.csv', index=False, encoding='utf-8')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example tokenizer\n",
    "def simple_tokenizer(text):\n",
    "    # Split by spaces and keep punctuation\n",
    "    return re.findall(r'\\w+|[^\\w\\s]', text, re.UNICODE)\n",
    "\n",
    "# Labeling function\n",
    "def label_tokens(tokens):\n",
    "    labels = []\n",
    "    for token in tokens:\n",
    "        if token.lower() in [\"rechargeable\", \"electric\", \"foot\", \"callus\", \"remover\", \"kit\", \"baby\", \"3in1\", \"double\", \"bottle\", \"warmer\"]:\n",
    "            if labels and labels[-1].startswith(\"B-\"):\n",
    "                labels.append(\"I-Product\")\n",
    "            else:\n",
    "                labels.append(\"B-Product\")\n",
    "        elif re.match(r'^\\d+$', token) or token in [\"ብር\", \"price\"]:\n",
    "            if labels and labels[-1].startswith(\"B-\"):\n",
    "                labels.append(\"I-PRICE\")\n",
    "            else:\n",
    "                labels.append(\"B-PRICE\")\n",
    "        elif token in [\"አዲስ\", \"አበባ\"]:\n",
    "            if labels and labels[-1].startswith(\"B-\"):\n",
    "                labels.append(\"I-LOC\")\n",
    "            else:\n",
    "                labels.append(\"B-LOC\")\n",
    "        else:\n",
    "            labels.append(\"O\")\n",
    "    return labels\n",
    "\n",
    "# Generate CoNLL format\n",
    "conll_output = []\n",
    "for _, row in raw_data.iterrows():\n",
    "    tokens = simple_tokenizer(row[\"text\"])\n",
    "    labels = label_tokens(tokens)\n",
    "    for token, label in zip(tokens, labels):\n",
    "        conll_output.append(f\"{token} {label}\")\n",
    "    conll_output.append(\"\")  # Blank line to separate messages\n",
    "\n",
    "# Write to .conll file\n",
    "output_file = \"labeled_data.conll\"\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"\\n\".join(conll_output))\n",
    "\n",
    "print(f\"CoNLL output saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 3: Fine-Tune an NER Model Using Hugging Face transformers for fine-tuning.\n",
    "\n",
    "Install Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import XLMRobertaTokenizer, XLMRobertaForTokenClassification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "import pandas as pd\n",
    "\n",
    "# Load the labeled data\n",
    "df = pd.read_csv('labeled_data.conll', sep=' ', header=None, names=['text', 'labels'], on_bad_lines='skip')  \n",
    "# Assuming space as delimiter, no header, assigning column names, skipping bad lines\n",
    "\n",
    "# Create a tokenizer\n",
    "tokenizer = XLMRobertaTokenizer.from_pretrained('xlm-roberta-base')\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "train_text, val_text, train_labels, val_labels = train_test_split(\n",
    "    df['text'].tolist(), df['labels'].tolist(), test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Define a label mapping (string to integer)\n",
    "label_map = {\n",
    "    'O': 0,\n",
    "    'B-Product': 1,\n",
    "    'I-Product': 2,\n",
    "    'B-PRICE': 3,\n",
    "    'I-PRICE': 4,\n",
    "    'B-LOC': 5,\n",
    "    'I-LOC': 6,\n",
    "    # Add any other labels you have\n",
    "}\n",
    "\n",
    "# Create a custom dataset class\n",
    "class NERDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, text, labels, tokenizer):\n",
    "        self.text = text\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.text[idx]\n",
    "        labels = self.labels[idx]\n",
    "\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=512,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "            padding='max_length',  # Add padding to max_length\n",
    "            truncation=True       # Truncate if exceeds max_length\n",
    "        )\n",
    "        # Convert string labels to numerical IDs using label_map\n",
    "        numerical_labels = [label_map.get(label, label_map['O']) for label in labels]  \n",
    "\n",
    "        # Pad labels to max_length\n",
    "        numerical_labels = numerical_labels + [label_map['O']] * (512 - len(numerical_labels)) \n",
    "        numerical_labels = numerical_labels[:512]  # Truncate if needed\n",
    "\n",
    "\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(numerical_labels)\n",
    "        }\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text)\n",
    "\n",
    "# Create dataset and data loader\n",
    "train_dataset = NERDataset(train_text, train_labels, tokenizer)\n",
    "val_dataset = NERDataset(val_text, val_labels, tokenizer)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "# Load the pre-trained model\n",
    "model = XLMRobertaForTokenClassification.from_pretrained('xlm-roberta-base', num_labels=8)\n",
    "\n",
    "# Set the device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "# Define the optimizer and scheduler\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1)\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(5):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in train_loader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    scheduler.step()\n",
    "    print(f'Epoch {epoch+1}, Loss: {total_loss / len(train_loader)}')\n",
    "\n",
    "    model.eval()\n",
    "    total_correct = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            logits = outputs.logits\n",
    "            _, predicted = torch.max(logits, dim=1)\n",
    "            total_correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = total_correct / len(val_loader)\n",
    "    print(f'Epoch {epoch+1}, Val Accuracy: {accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 4: Model Comparison & Selection To compare different models and select the best-performing one, you can use the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import XLMRobertaTokenizer, XLMRobertaForTokenClassification, DistilBertTokenizer, DistilBertForTokenClassification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "import pandas as pd\n",
    "\n",
    "# Load the labeled data (This line is added to define df)\n",
    "df = pd.read_csv('labeled_data.conll', sep=' ', header=None, names=['text', 'labels'], on_bad_lines='skip')  \n",
    "# Assuming space as delimiter, no header, assigning column names, skipping bad lines\n",
    "\n",
    "\n",
    "# Load the labeled data\n",
    "tokenizer = XLMRobertaTokenizer.from_pretrained('xlm-roberta-base')\n",
    "\n",
    "# Create a tokenizer\n",
    "tokenizer = XLMRobertaTokenizer.from_pretrained('xlm-roberta-base')\n",
    "\n",
    "# Reset index after splitting to ensure continuous indices\n",
    "train_text, val_text, train_labels, val_labels = train_test_split(df['text'], df['labels'], test_size=0.2, random_state=42)\n",
    "train_text = train_text.reset_index(drop=True)  # Reset index for train_text\n",
    "train_labels = train_labels.reset_index(drop=True)  # Reset index for train_labels\n",
    "val_text = val_text.reset_index(drop=True)  # Reset index for val_text\n",
    "val_labels = val_labels.reset_index(drop=True)  # Reset index for val_labels\n",
    "\n",
    "\n",
    "# Create a custom dataset class\n",
    "class NERDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, text, labels, tokenizer):\n",
    "        self.text = text\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.text[idx]  # Access using the reset index\n",
    "        labels = self.labels[idx]  # Access using the reset index\n",
    "\n",
    "        # Define a label mapping (string to integer)\n",
    "        label_map = {\n",
    "            'O': 0,\n",
    "            'B-Product': 1,\n",
    "            'I-Product': 2,\n",
    "            'B-PRICE': 3,\n",
    "            'I-PRICE': 4,\n",
    "            'B-LOC': 5,\n",
    "            'I-LOC': 6,\n",
    "            # Add any other labels you have\n",
    "        }\n",
    "\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=512,  # Set max_length\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "            padding='max_length', # Add padding\n",
    "            truncation=True      # Add truncation\n",
    "        )\n",
    "\n",
    "        # Convert string labels to numerical IDs using label_map\n",
    "        numerical_labels = [label_map.get(label, label_map['O']) for label in labels]  \n",
    "\n",
    "        # Pad and truncate labels to match input_ids length\n",
    "        numerical_labels = numerical_labels + [label_map['O']] * (encoding['input_ids'].shape[1] - len(numerical_labels))  \n",
    "        numerical_labels = numerical_labels[:encoding['input_ids'].shape[1]] \n",
    "\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(numerical_labels) \n",
    "        }\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text)\n",
    "\n",
    "# Create dataset and data loader\n",
    "train_dataset = NERDataset(train_text, train_labels, tokenizer)\n",
    "val_dataset = NERDataset(val_text, val_labels, tokenizer)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "\n",
    "# Load the pre-trained models\n",
    "models = {\n",
    "    'xlm-roberta': XLMRobertaForTokenClassification.from_pretrained('xlm-roberta-base', num_labels=8),\n",
    "    'distilbert': DistilBertForTokenClassification.from_pretrained('distilbert-base-uncased', num_labels=8)\n",
    "}\n",
    "\n",
    "# Set the device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Train and evaluate each model\n",
    "for model_name, model in models.items():\n",
    "    model.to(device)\n",
    "\n",
    "    # Define the optimizer and scheduler\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1)\n",
    "\n",
    "    # Train the model\n",
    "    for epoch in range(5):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for batch in train_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        scheduler.step()\n",
    "        print(f'Epoch {epoch+1}, Loss: {total_loss / len(train_loader)}')\n",
    "\n",
    "        model.eval()\n",
    "        total_correct = 0\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                labels = batch['labels'].to(device)\n",
    "\n",
    "                outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "                logits = outputs.logits\n",
    "                _, predicted = torch.max(logits, dim=1)\n",
    "                total_correct += (predicted == labels).sum().item()\n",
    "\n",
    "        accuracy = total_correct / len(val_loader)\n",
    "        print(f'Epoch {epoch+1}, Val Accuracy: {accuracy:.4f}')\n",
    "\n",
    "    # Evaluate the model on the validation set\n",
    "    model.eval()\n",
    "    total_correct = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            logits = outputs.logits\n",
    "            _, predicted = torch.max(logits, dim=1)\n",
    "            total_correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = total_correct / len(val_loader)\n",
    "    print(f'{model_name} Val Accuracy: {accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 5: Model Interpretability To use model interpretability tools to explain how the NER model identifies entities, ensuring transparency and trust in the system, you can use the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install shap --upgrade\n",
    "import shap\n",
    "!pip install lime\n",
    "from lime.lime_text import LimeTextExplainer\n",
    "!pip install transformers datasets seqeval\n",
    "import torch\n",
    "from transformers import XLMRobertaTokenizer, XLMRobertaForTokenClassification # Import XLMRobertaForTokenClassification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Load the pre-trained model\n",
    "model = XLMRobertaForTokenClassification.from_pretrained('xlm-roberta-base', num_labels=8)\n",
    "\n",
    "# Load the labeled data with space as delimiter\n",
    "df = pd.read_csv('labeled_data.conll', sep=' ', header=None, names=['text', 'labels'], on_bad_lines='skip')\n",
    "\n",
    "\n",
    "# Create dataset and data loader\n",
    "# Define the tokenizer here, outside the loop, to make it accessible later\n",
    "tokenizer = XLMRobertaTokenizer.from_pretrained('xlm-roberta-base')  \n",
    "train_dataset = NERDataset(df['text'], df['labels'], tokenizer)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "# Preprocessing to extract text and labels\n",
    "# Assuming your 'labeled_data.conll' has the format: \"token label\" per line\n",
    "# and empty lines separating sentences\n",
    "\n",
    "all_texts = []\n",
    "all_labels = []\n",
    "current_text = []\n",
    "current_labels = []\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "  # Modified condition to check for NaN in 'text' column which indicates a new sentence\n",
    "  if pd.isnull(row['text']) or row['text'] == '': # Check for empty lines indicating sentence end or an empty string\n",
    "    if current_text: #Only append if current_text is not empty\n",
    "        all_texts.append(\" \".join(current_text)) # Join tokens into a sentence\n",
    "        all_labels.append(current_labels)\n",
    "        current_text = []\n",
    "        current_labels = []\n",
    "  else: \n",
    "    try:\n",
    "      token, label = row['text'].split(\" \", 1)  # Split into token and label\n",
    "      current_text.append(token)\n",
    "      current_labels.append(label)\n",
    "    except ValueError:\n",
    "      pass # Skip lines that can't be split, you might need to handle these better\n",
    "\n",
    "\n",
    "# Check if the last sentence was not added (due to no empty line at the end)\n",
    "if current_text:\n",
    "    all_texts.append(\" \".join(current_text))\n",
    "    all_labels.append(current_labels)\n",
    "\n",
    "# Convert lists to DataFrame\n",
    "df = pd.DataFrame({'text': all_texts, 'labels': all_labels})\n",
    "\n",
    "\n",
    "# Create a custom dataset class\n",
    "class NERDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, text, labels, tokenizer):\n",
    "        self.text = text\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        # Define a label mapping (string to integer)\n",
    "        self.label_map = {\n",
    "            'O': 0,\n",
    "            'B-Product': 1,\n",
    "            'I-Product': 2,\n",
    "            'B-PRICE': 3,\n",
    "            'I-PRICE': 4,\n",
    "            'B-LOC': 5,\n",
    "            'I-LOC': 6,\n",
    "            # Add any other labels you have\n",
    "        }\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.text[idx]\n",
    "        labels = self.labels[idx]\n",
    "\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=512,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "            padding='max_length',\n",
    "            truncation=True\n",
    "        )\n",
    "        # Convert string labels to numerical IDs using label_map\n",
    "        numerical_labels = [self.label_map.get(label, self.label_map['O']) for label in labels]  \n",
    "        # Pad labels to max_length\n",
    "        numerical_labels = numerical_labels + [self.label_map['O']] * (512 - len(numerical_labels)) \n",
    "        numerical_labels = numerical_labels[:512]  # Truncate if needed\n",
    "\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            # Change: Create tensor from numerical_labels, not labels\n",
    "            'labels': torch.tensor(numerical_labels) \n",
    "        }\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text)\n",
    "\n",
    "# Create dataset and data loader\n",
    "train_dataset = NERDataset(df['text'], df['labels'], XLMRobertaTokenizer.from_pretrained('xlm-roberta-base'))\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "# Get a sample of data for the masker\n",
    "for batch in train_loader:\n",
    "    # Access the text using the dataset index instead of batch\n",
    "    #input_texts = [train_dataset.text[i] for i in batch['input_ids']] # Modified line \n",
    "    input_texts = [train_dataset.text[i] for i in range(len(batch['input_ids']))]\n",
    "    break  # Get only one batch for the masker\n",
    "\n",
    "# Create a masker using the input_texts\n",
    "masker = shap.maskers.Text(tokenizer, mask_token=\"[MASK]\") # Use the decoded text for the masker\n",
    "\n",
    "# Create a SHAP explainer with the masker\n",
    "# Adjust output_names according to your label mapping\n",
    "output_names = list(train_dataset.label_map.keys())  # Getting output names from label_map\n",
    "\n",
    "# Instead of TextExplainer or KernelExplainer, use PartitionExplainer:\n",
    "explainer = shap.explainers.PartitionExplainer(model, masker, output_names=output_names)  \n",
    "\n",
    "# Create a LIME explainer\n",
    "lime_explainer = LimeTextExplainer()\n",
    "\n",
    "# Explain the model's predictions\n",
    "for batch in train_loader:\n",
    "    input_ids = batch['input_ids']\n",
    "    attention_mask = batch['attention_mask']\n",
    "    labels = batch['labels']\n",
    "\n",
    "    # Get the model's predictions\n",
    "    outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "    logits = outputs.logits\n",
    "    _, predicted = torch.max(logits, dim=1)\n",
    "\n",
    "    # Explain the model's predictions using SHAP\n",
    "    # Change: Convert input_ids to NumPy array\n",
    "    input_ids_np = input_ids.cpu().numpy() \n",
    "    shap_values = explainer(input_ids_np) \n",
    "\n",
    "    # Change: Get the shap values for the first prediction\n",
    "    shap_values_first_prediction = shap_values.values[0]\n",
    "    shap.plots.text(shap_values_first_prediction) \n",
    "    print(shap_values)\n",
    "\n",
    "    # Explain the model's predictions using LIME\n",
    "    # Change: Convert predicted to NumPy array and get the first prediction\n",
    "    predicted_np = predicted.cpu().numpy()[0]\n",
    "    lime_explanations = lime_explainer.explain_instance(predicted_np, input_ids)\n",
    "    print(lime_explanations)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "EthioMart",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
