{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 3: Fine-Tune an NER Model Using Hugging Face transformers for fine-tuning.\n",
    "\n",
    "Install Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import XLMRobertaTokenizer, XLMRobertaForTokenClassification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "import pandas as pd\n",
    "\n",
    "# Load the labeled data\n",
    "df = pd.read_csv('labeled_data.conll', sep=' ', header=None, names=['text', 'labels'], on_bad_lines='skip')  \n",
    "# Assuming space as delimiter, no header, assigning column names, skipping bad lines\n",
    "\n",
    "# Create a tokenizer\n",
    "tokenizer = XLMRobertaTokenizer.from_pretrained('xlm-roberta-base')\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "train_text, val_text, train_labels, val_labels = train_test_split(\n",
    "    df['text'].tolist(), df['labels'].tolist(), test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Define a label mapping (string to integer)\n",
    "label_map = {\n",
    "    'O': 0,\n",
    "    'B-Product': 1,\n",
    "    'I-Product': 2,\n",
    "    'B-PRICE': 3,\n",
    "    'I-PRICE': 4,\n",
    "    'B-LOC': 5,\n",
    "    'I-LOC': 6,\n",
    "    # Add any other labels you have\n",
    "}\n",
    "\n",
    "# Create a custom dataset class\n",
    "class NERDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, text, labels, tokenizer):\n",
    "        self.text = text\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.text[idx]\n",
    "        labels = self.labels[idx]\n",
    "\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=512,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "            padding='max_length',  # Add padding to max_length\n",
    "            truncation=True       # Truncate if exceeds max_length\n",
    "        )\n",
    "        # Convert string labels to numerical IDs using label_map\n",
    "        numerical_labels = [label_map.get(label, label_map['O']) for label in labels]  \n",
    "\n",
    "        # Pad labels to max_length\n",
    "        numerical_labels = numerical_labels + [label_map['O']] * (512 - len(numerical_labels)) \n",
    "        numerical_labels = numerical_labels[:512]  # Truncate if needed\n",
    "\n",
    "\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(numerical_labels)\n",
    "        }\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text)\n",
    "\n",
    "# Create dataset and data loader\n",
    "train_dataset = NERDataset(train_text, train_labels, tokenizer)\n",
    "val_dataset = NERDataset(val_text, val_labels, tokenizer)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "# Load the pre-trained model\n",
    "model = XLMRobertaForTokenClassification.from_pretrained('xlm-roberta-base', num_labels=8)\n",
    "\n",
    "# Set the device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "# Define the optimizer and scheduler\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1)\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(5):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in train_loader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    scheduler.step()\n",
    "    print(f'Epoch {epoch+1}, Loss: {total_loss / len(train_loader)}')\n",
    "\n",
    "    model.eval()\n",
    "    total_correct = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            logits = outputs.logits\n",
    "            _, predicted = torch.max(logits, dim=1)\n",
    "            total_correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = total_correct / len(val_loader)\n",
    "    print(f'Epoch {epoch+1}, Val Accuracy: {accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 4: Model Comparison & Selection To compare different models and select the best-performing one, you can use the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import XLMRobertaTokenizer, XLMRobertaForTokenClassification, DistilBertTokenizer, DistilBertForTokenClassification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "import pandas as pd\n",
    "\n",
    "# Load the labeled data (This line is added to define df)\n",
    "df = pd.read_csv('labeled_data.conll', sep=' ', header=None, names=['text', 'labels'], on_bad_lines='skip')  \n",
    "# Assuming space as delimiter, no header, assigning column names, skipping bad lines\n",
    "\n",
    "\n",
    "# Load the labeled data\n",
    "tokenizer = XLMRobertaTokenizer.from_pretrained('xlm-roberta-base')\n",
    "\n",
    "# Create a tokenizer\n",
    "tokenizer = XLMRobertaTokenizer.from_pretrained('xlm-roberta-base')\n",
    "\n",
    "# Reset index after splitting to ensure continuous indices\n",
    "train_text, val_text, train_labels, val_labels = train_test_split(df['text'], df['labels'], test_size=0.2, random_state=42)\n",
    "train_text = train_text.reset_index(drop=True)  # Reset index for train_text\n",
    "train_labels = train_labels.reset_index(drop=True)  # Reset index for train_labels\n",
    "val_text = val_text.reset_index(drop=True)  # Reset index for val_text\n",
    "val_labels = val_labels.reset_index(drop=True)  # Reset index for val_labels\n",
    "\n",
    "\n",
    "# Create a custom dataset class\n",
    "class NERDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, text, labels, tokenizer):\n",
    "        self.text = text\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.text[idx]  # Access using the reset index\n",
    "        labels = self.labels[idx]  # Access using the reset index\n",
    "\n",
    "        # Define a label mapping (string to integer)\n",
    "        label_map = {\n",
    "            'O': 0,\n",
    "            'B-Product': 1,\n",
    "            'I-Product': 2,\n",
    "            'B-PRICE': 3,\n",
    "            'I-PRICE': 4,\n",
    "            'B-LOC': 5,\n",
    "            'I-LOC': 6,\n",
    "            # Add any other labels you have\n",
    "        }\n",
    "\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=512,  # Set max_length\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "            padding='max_length', # Add padding\n",
    "            truncation=True      # Add truncation\n",
    "        )\n",
    "\n",
    "        # Convert string labels to numerical IDs using label_map\n",
    "        numerical_labels = [label_map.get(label, label_map['O']) for label in labels]  \n",
    "\n",
    "        # Pad and truncate labels to match input_ids length\n",
    "        numerical_labels = numerical_labels + [label_map['O']] * (encoding['input_ids'].shape[1] - len(numerical_labels))  \n",
    "        numerical_labels = numerical_labels[:encoding['input_ids'].shape[1]] \n",
    "\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(numerical_labels) \n",
    "        }\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text)\n",
    "\n",
    "# Create dataset and data loader\n",
    "train_dataset = NERDataset(train_text, train_labels, tokenizer)\n",
    "val_dataset = NERDataset(val_text, val_labels, tokenizer)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "\n",
    "# Load the pre-trained models\n",
    "models = {\n",
    "    'xlm-roberta': XLMRobertaForTokenClassification.from_pretrained('xlm-roberta-base', num_labels=8),\n",
    "    'distilbert': DistilBertForTokenClassification.from_pretrained('distilbert-base-uncased', num_labels=8)\n",
    "}\n",
    "\n",
    "# Set the device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Train and evaluate each model\n",
    "for model_name, model in models.items():\n",
    "    model.to(device)\n",
    "\n",
    "    # Define the optimizer and scheduler\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1)\n",
    "\n",
    "    # Train the model\n",
    "    for epoch in range(5):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for batch in train_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        scheduler.step()\n",
    "        print(f'Epoch {epoch+1}, Loss: {total_loss / len(train_loader)}')\n",
    "\n",
    "        model.eval()\n",
    "        total_correct = 0\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                labels = batch['labels'].to(device)\n",
    "\n",
    "                outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "                logits = outputs.logits\n",
    "                _, predicted = torch.max(logits, dim=1)\n",
    "                total_correct += (predicted == labels).sum().item()\n",
    "\n",
    "        accuracy = total_correct / len(val_loader)\n",
    "        print(f'Epoch {epoch+1}, Val Accuracy: {accuracy:.4f}')\n",
    "\n",
    "    # Evaluate the model on the validation set\n",
    "    model.eval()\n",
    "    total_correct = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            logits = outputs.logits\n",
    "            _, predicted = torch.max(logits, dim=1)\n",
    "            total_correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = total_correct / len(val_loader)\n",
    "    print(f'{model_name} Val Accuracy: {accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 5: Model Interpretability To use model interpretability tools to explain how the NER model identifies entities, ensuring transparency and trust in the system, you can use the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install shap --upgrade\n",
    "import shap\n",
    "pip install lime\n",
    "from lime.lime_text import LimeTextExplainer\n",
    "pip install transformers datasets seqeval\n",
    "import torch\n",
    "from transformers import XLMRobertaTokenizer, XLMRobertaForTokenClassification # Import XLMRobertaForTokenClassification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Load the pre-trained model\n",
    "model = XLMRobertaForTokenClassification.from_pretrained('xlm-roberta-base', num_labels=8)\n",
    "\n",
    "# Load the labeled data with space as delimiter\n",
    "df = pd.read_csv('labeled_data.conll', sep=' ', header=None, names=['text', 'labels'], on_bad_lines='skip')\n",
    "\n",
    "\n",
    "# Create dataset and data loader\n",
    "# Define the tokenizer here, outside the loop, to make it accessible later\n",
    "tokenizer = XLMRobertaTokenizer.from_pretrained('xlm-roberta-base')  \n",
    "train_dataset = NERDataset(df['text'], df['labels'], tokenizer)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "# Preprocessing to extract text and labels\n",
    "# Assuming your 'labeled_data.conll' has the format: \"token label\" per line\n",
    "# and empty lines separating sentences\n",
    "\n",
    "all_texts = []\n",
    "all_labels = []\n",
    "current_text = []\n",
    "current_labels = []\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "  # Modified condition to check for NaN in 'text' column which indicates a new sentence\n",
    "  if pd.isnull(row['text']) or row['text'] == '': # Check for empty lines indicating sentence end or an empty string\n",
    "    if current_text: #Only append if current_text is not empty\n",
    "        all_texts.append(\" \".join(current_text)) # Join tokens into a sentence\n",
    "        all_labels.append(current_labels)\n",
    "        current_text = []\n",
    "        current_labels = []\n",
    "  else: \n",
    "    try:\n",
    "      token, label = row['text'].split(\" \", 1)  # Split into token and label\n",
    "      current_text.append(token)\n",
    "      current_labels.append(label)\n",
    "    except ValueError:\n",
    "      pass # Skip lines that can't be split, you might need to handle these better\n",
    "\n",
    "\n",
    "# Check if the last sentence was not added (due to no empty line at the end)\n",
    "if current_text:\n",
    "    all_texts.append(\" \".join(current_text))\n",
    "    all_labels.append(current_labels)\n",
    "\n",
    "# Convert lists to DataFrame\n",
    "df = pd.DataFrame({'text': all_texts, 'labels': all_labels})\n",
    "\n",
    "\n",
    "# Create a custom dataset class\n",
    "class NERDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, text, labels, tokenizer):\n",
    "        self.text = text\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        # Define a label mapping (string to integer)\n",
    "        self.label_map = {\n",
    "            'O': 0,\n",
    "            'B-Product': 1,\n",
    "            'I-Product': 2,\n",
    "            'B-PRICE': 3,\n",
    "            'I-PRICE': 4,\n",
    "            'B-LOC': 5,\n",
    "            'I-LOC': 6,\n",
    "            # Add any other labels you have\n",
    "        }\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.text[idx]\n",
    "        labels = self.labels[idx]\n",
    "\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=512,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "            padding='max_length',\n",
    "            truncation=True\n",
    "        )\n",
    "        # Convert string labels to numerical IDs using label_map\n",
    "        numerical_labels = [self.label_map.get(label, self.label_map['O']) for label in labels]  \n",
    "        # Pad labels to max_length\n",
    "        numerical_labels = numerical_labels + [self.label_map['O']] * (512 - len(numerical_labels)) \n",
    "        numerical_labels = numerical_labels[:512]  # Truncate if needed\n",
    "\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            # Change: Create tensor from numerical_labels, not labels\n",
    "            'labels': torch.tensor(numerical_labels) \n",
    "        }\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text)\n",
    "\n",
    "# Create dataset and data loader\n",
    "train_dataset = NERDataset(df['text'], df['labels'], XLMRobertaTokenizer.from_pretrained('xlm-roberta-base'))\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "# Get a sample of data for the masker\n",
    "for batch in train_loader:\n",
    "    # Access the text using the dataset index instead of batch\n",
    "    #input_texts = [train_dataset.text[i] for i in batch['input_ids']] # Modified line \n",
    "    input_texts = [train_dataset.text[i] for i in range(len(batch['input_ids']))]\n",
    "    break  # Get only one batch for the masker\n",
    "\n",
    "# Create a masker using the input_texts\n",
    "masker = shap.maskers.Text(tokenizer, mask_token=\"[MASK]\") # Use the decoded text for the masker\n",
    "\n",
    "# Create a SHAP explainer with the masker\n",
    "# Adjust output_names according to your label mapping\n",
    "output_names = list(train_dataset.label_map.keys())  # Getting output names from label_map\n",
    "\n",
    "# Instead of TextExplainer or KernelExplainer, use PartitionExplainer:\n",
    "explainer = shap.explainers.PartitionExplainer(model, masker, output_names=output_names)  \n",
    "\n",
    "# Create a LIME explainer\n",
    "lime_explainer = LimeTextExplainer()\n",
    "\n",
    "# Explain the model's predictions\n",
    "for batch in train_loader:\n",
    "    input_ids = batch['input_ids']\n",
    "    attention_mask = batch['attention_mask']\n",
    "    labels = batch['labels']\n",
    "\n",
    "    # Get the model's predictions\n",
    "    outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "    logits = outputs.logits\n",
    "    _, predicted = torch.max(logits, dim=1)\n",
    "\n",
    "    # Explain the model's predictions using SHAP\n",
    "    # Change: Convert input_ids to NumPy array\n",
    "    input_ids_np = input_ids.cpu().numpy() \n",
    "    shap_values = explainer(input_ids_np) \n",
    "\n",
    "    # Change: Get the shap values for the first prediction\n",
    "    shap_values_first_prediction = shap_values.values[0]\n",
    "    shap.plots.text(shap_values_first_prediction) \n",
    "    print(shap_values)\n",
    "\n",
    "    # Explain the model's predictions using LIME\n",
    "    # Change: Convert predicted to NumPy array and get the first prediction\n",
    "    predicted_np = predicted.cpu().numpy()[0]\n",
    "    lime_explanations = lime_explainer.explain_instance(predicted_np, input_ids)\n",
    "    print(lime_explanations)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "EthioMart",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
